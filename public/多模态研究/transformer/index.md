# Transformer



## transformer
方法来自于论文《Attention is All You Need》,之前看b站的各种视频讲解总是一知半解，现在希望可以通过阅读原始论文弄懂。


## abstract
首先让我们阅读文章的摘要。
![摘要](/pic/transformer/摘要.png)

摘要提到之前的主流序列转导模型主要是rnn，使用编码器-解码器结构，使用注意力机制进行增强。而作者提出了一种新的简单的网络架构名为"transformer"，而这种架构有三个特点：
1. 完全摒弃了RNN/CNN
2. 仍然使用编码器-解码器结构
3. 完全基于注意力机制


## backgroud其中的RNN与编码器-解码器结构

![RNN](/pic/transformer/rnn.png)
经典的RNN结构，这里不再赘述，简单理解便是上一个token的计算结果会作为第二个token的输入来参与计算。不过RNN有很多缺陷，比如输入和输出不等长。

![编码器-解码器](/pic/transformer/编码器-解码器.png)
编码器-解码器结构可以理解为将RNN的上半部分和下半部分拆开，编码器去掉了RNN上半部分y的输出，只保留隐藏状态h，中间向量C（上下文向量）是对整个输入序列的语义编码，是一个固定长度的向量，涵盖了整个输出文本的语义信息。

不过编码器-解码器也有很多问题：
1. 语义的稀释问题，随着序列长度的增长，远距离依赖信息在传递过程中很容易被稀释，导致模型对长距离依赖关系的建模能力减弱。
2. 不同token对结果的重要性应该是不一样的，但是这个结构无法体现。

> 注意注意力机制在之前就已经提出过，该论文是提出了自注意力机制。

[!注意力机制](/pic/transformer/注意力机制.png)
最简单的注意力机制其实就是一个权重的分配，每一个输出的输入是通过加权平均得到的。



## 模型架构
[!模型架构](/pic/transformer/模型架构.png)

要理解transformer的架构设计，要理解该架构解决的问题：串行计算。对于文本“我爱水课”来说，假设编码“水”这个token，在之前必须要先等“我爱”编码完，但是实际上机器学习的训练我们是有答案的，“我爱水课”与“I love easy courses”我们事先都是知道的，所以实际上没有必要去等待。我们不让他们再去等待，而是采用另一种编码-解码方式。

在编码的时候，首先每一个词都会被转换为一个向量，在transformer中，该向量为512维，假设在编码“水”的时候，可以以全局的视角看到上下文所有的信息，然后将这些信息编码到向量中，而且这个过程是可以并行进行的。（解码同理）


模型架构的左边是编码器，右边是解码器。具体的流程如下：
1. 对于“我爱水课”这句话，首先将每个词转换为512维的向量（词嵌入embedding），“水”和“瀑布”可能被映射到很近的向量空间。词嵌入有不同的方式，词嵌入模型可以一起去训练，你也可以用别人训练好的词嵌入模型。
2. 位置编码。通过显式的编码将位置信息带进去。给每一个词生成一个512维的位置向量（具体是通过余弦向量），然后将位置向量直接叠加到原本的语义信息中。
3. 单头注意力。主要是为了处理上下文编码（橙色模块），先讲单头注意力，有三个矩阵wq，wk，wv，每一个矩阵都是512×512的矩阵。假设“水”这个词，这个向量分别×wq，wv，wk，1×512 × 515×512 = 1 × 512，所以得到三个1×512，注意这三个矩阵是在训练过程中得到的（非常重要的参数）。每一个词都会得到三个矩阵，q是qurey，k是key，v是value。“我爱水课”和“西瓜是我最爱的水果”其中的水应该是不一样的，它的偏移应该不一样，对于qkv来说，可以理解为一个词的q是在发问：是否有名词？k则是在回答：我是不是名词，而v则是该名词的值。计算公式如下：

```latex
Attention(Q,K,V)=softmax(\frac{Qk^T}{\sqrt{d_{k} } } )V
```

Q与K做点积，以水为例，Q水×（K我，k爱，k水，k课）得到的结果理解为水向每一个token发问，拿到结果，如果目标是名词的话，那么这个值就会越大，这个权重实际上就是一个相似度分数，然后再做一个缩放便于模型训练，再经过softmax，得到的含义就是当前所有词对水这个词语义影响的重要程度，然后再与v做矩阵乘法便得到了全新的信息向量。

4. 多头注意力机制。实际上transformer使用的是多头注意力机制，

---

> Author: <no value>  
> URL: http://localhost:1313/%E5%A4%9A%E6%A8%A1%E6%80%81%E7%A0%94%E7%A9%B6/transformer/  

